
\documentclass[11pt]{scrartcl}
\usepackage{graphicx}
\usepackage{amsmath, amssymb, textcomp}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{ucs}
\usepackage[T1]{fontenc}
\usepackage{subcaption} 	
\usepackage{hyperref}
\usepackage{fullpage}	
\usepackage{color}
\usepackage{enumitem}
\setlist[itemize]{noitemsep, topsep=1pt}
\usepackage[font=footnotesize]{caption}
\usepackage{draftwatermark}
\SetWatermarkText{Martin Fan√©}
\SetWatermarkScale{3}
\SetWatermarkColor[gray]{0.99}
\usepackage[numindex,nottoc,section]{} 


\def\Reg{\textsuperscript{\textregistered}}
\def\TM{\textsuperscript{\texttrademark}}
\def\CR{\textsuperscript{\textcopyright}}	


\usepackage{listings, xcolor}
\lstset
{	
	%	language     = Docker,
	basicstyle   = \ttfamily,
	keywordstyle = \color{blue}\ttfamily,
	stringstyle  = \color{orange}\ttfamily,
	commentstyle = \color{green}\ttfamily,
	morecomment  = [l][\color{teal}]{\#},
	%	backgroundcolor = \color{lightgray}\ttfamily,
	breaklines=true,
	tabsize=3,
	%	numbers=left,
	numberstyle=\color{black},
	numbersep=5pt, 
	frame=single
}

\begin{document}
\tableofcontents

\newpage
\section{Fundamentals}
\begin{itemize}
	\item LLM: Model trained on massive datasets to achieve advanced language processing capabilities
	\item Foundation Models: Large ML model trained on vast amount of data \& fine-tuned for more specific language understanding and generation tasks 
\end{itemize}

\textbf{Components: }
\begin{itemize}
	\item Encoder: Converts text input into tokens (embeddings)
	\item Decoder: Converts generated output tokens back into meaningful words
	\item Transformer: Train the token embeddings
\end{itemize}

\textbf{Databricks AI:}
\begin{itemize}
	\item GenAI (Custom models, model serving, RAG)
	\item End-to-end AI (MLOps with MLFlow, AutoML, Monitoring, Governance)
	\item Databricks + MosaicML:
	\begin{itemize}
		\item Rapid democratization of model capabilities
		\item Making GenAI models work for enterprises
		\item Unifying AI and data stack
		\item Advantages: Customize models, secure environment, competitive
	\end{itemize}
\end{itemize}

\textbf{Legal and Ethical Considerations:}
\begin{itemize}
	\item Prompt Injection: Inserting a specific instruction or prompt within the input text to manipulate the normal behavior of LLMs
	\item Prompt Engineering: Designing and crafting effective prompts or instructions for generating desired outputs from an LLM
\end{itemize}


\newpage
\section{Solution Development}
\textbf{From Prompt Engineering to RAG:}
\begin{itemize}
	\item Prompt: Input or query given to a LLM to elicit a specific response
	\item Prompt engineering: Practice of designing and refining prompts to optimize the responses generated by an AI model
	\item Prompt components: Instruction, context, input/question, output type/format
	\item Prompt techniques:
	\begin{itemize}
		\item Zero-shot: Prompt that generates text or performs a task without providing any examples or additional training specified to that task
		\item Few-shot: Prompt provides with a few input-output examples to guide the model for generating the desired output
	\end{itemize}
	\item Prompt chaining: Multiple tasks are linked together, with the output of one prompt serving as the input for the next $\to$ allows for more complex tasks to be broken down into manageable steps
	\item Chain-of-thought (CoT) prompting: Enhances the reasoning capabilities of LLMs by guiding them to articulate their though processes step-by-step
\end{itemize}

\textbf{Tips and tricks:}
\begin{itemize}
	\item Different models may require different prompts
	\item Provide examples and clues to guide model's response generation
	\item Different use cases may require different prompts
	\item Use delimiters to distinguish between instruction and context
	\item Ask the model to return structured output
	\item Ask the model not to hallucinate, not to assume, not to rush
\end{itemize}

\textbf{RAG:}
\begin{itemize}
	\item Passing context as model inputs improves factual recall
	\item Retrieve data/documents relevant to a question/task, provide them as context to augment the prompts to an LLM to improve generation
	\item Components:
	\begin{itemize}
		\item Index \& embed: Embedding model creates vector representations of the documents and the user query
		\item Vector store: Specialized to store unstructured data indexed by vectors
		\item Retrieval: Search stored vectors using similarity search to retrieve relevant information
		\item Filtering \& Reranking: Process of selecting or ranking retrieved documents before passing as context
		\item Prompt augmentation: Prompt engineering workflow to enhance context via injection of data retrieved from the vector store
		\item Generation
	\end{itemize}
	\item Flow: 
	\begin{itemize}
		\item Documents are embedded
		\item User asks question and LLM converts query to embeddings
		\item Similarity search for embedded documents and embedded user query
		\item Similar documents are passed as context to LLM generation model
		\item Model augments and generates completion
	\end{itemize}
	\item Databricks:
	\begin{itemize}
		\item DLT
		\item Mosaic AI Model Serving (embeddings, foundation models, custom models)
		\item Mosaic AI Vector Search
		\item Unity Catalog's Model registry
	\end{itemize}
	\item Benefits: 
	\begin{itemize}
		\item Up-to-date and accurate responses,
		\item Reducing inaccurate responses or hallucinations
		\item Domain-specific contextualization
		\item Efficiency and cost-effectiveness
	\end{itemize}
\end{itemize}

\textbf{Preparing Data for RAG Solution:}
\begin{itemize}
	\item Issues:
	\begin{itemize}
		\item Poor quality model output
		\item Lost in the middle paradigm
		\item Inefficient retrieval
		\item Wring embedding model
	\end{itemize}
	\item Data prep process:
	\begin{itemize}
		\item Ingestion $\to$ Data storage $\to$ Chunking $\to$ Persist in vector store
		\item Delta Lake for storing data and tables in the Databricks DI Platform
		\item Unity Catalog for governance, discovery, access control for RAG applications
		\item Document extraction: Split documents into chunks, embed chunks with a model, store them in a vector store
	\end{itemize}
	\item Chunking data:
	\begin{itemize}
		\item Context-aware chunking: By sentence/paragraph/section, leverage special punctuation, include metadata/tags/titles
		\item Fixed-size chunking: Divide by a specific number of tokens, simple and computationally cheap method
		\item Chunking overlap defines the amount of overlap between consecutive chunks ensuring that no contextual information is lost
		\item Window summarization: Each chunk includes a windowed summary of previous chunks
	\end{itemize}
	\item Embedding model:
	\begin{itemize}
		\item 
	\end{itemize}
\end{itemize}



\newpage
\section{Application Development}


\newpage
\section{Application Evaluation and Governance}


\newpage
\section{Application Deployment and Monitoring}

\end{document}