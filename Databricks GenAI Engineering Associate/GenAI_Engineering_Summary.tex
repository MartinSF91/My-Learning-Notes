
\documentclass[11pt]{scrartcl}
\usepackage{graphicx}
\usepackage{amsmath, amssymb, textcomp}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{ucs}
\usepackage[T1]{fontenc}
\usepackage{subcaption} 	
\usepackage{hyperref}
\usepackage{fullpage}	
\usepackage{color}
\usepackage{enumitem}
\setlist[itemize]{noitemsep, topsep=1pt}
\usepackage[font=footnotesize]{caption}
\usepackage{draftwatermark}
\SetWatermarkText{Martin Fan√©}
\SetWatermarkScale{3}
\SetWatermarkColor[gray]{0.99}
\usepackage[numindex,nottoc,section]{} 


\def\Reg{\textsuperscript{\textregistered}}
\def\TM{\textsuperscript{\texttrademark}}
\def\CR{\textsuperscript{\textcopyright}}	


\usepackage{listings, xcolor}
\lstset
{	
	%	language     = Docker,
	basicstyle   = \ttfamily,
	keywordstyle = \color{blue}\ttfamily,
	stringstyle  = \color{orange}\ttfamily,
	commentstyle = \color{green}\ttfamily,
	morecomment  = [l][\color{teal}]{\#},
	%	backgroundcolor = \color{lightgray}\ttfamily,
	breaklines=true,
	tabsize=3,
	%	numbers=left,
	numberstyle=\color{black},
	numbersep=5pt, 
	frame=single
}

\begin{document}
\tableofcontents

\newpage
\section{Fundamentals}
\begin{itemize}
	\item LLM: Model trained on massive datasets to achieve advanced language processing capabilities
	\item Foundation Models: Large ML model trained on vast amount of data \& fine-tuned for more specific language understanding and generation tasks 
	\item Components: 
	\begin{itemize}
		\item Encoder: Converts text input into tokens (embeddings)
		\item Decoder: Converts generated output tokens back into meaningful words
		\item Transformer: Train the token embeddings
	\end{itemize}
	\item Databricks AI:
	\begin{itemize}
		\item GenAI (Custom models, model serving, RAG)
		\item End-to-end AI (MLOps with MLFlow, AutoML, Monitoring, Governance)
		\item Databricks + MosaicML:
		\begin{itemize}
			\item Rapid democratization of model capabilities
			\item Making GenAI models work for enterprises
			\item Unifying AI and data stack
			\item Advantages: Customize models, secure environment, competitive
		\end{itemize}
	\end{itemize}
	\item Legal and Ethical Considerations:
	\begin{itemize}
		\item Prompt Injection: Inserting a specific instruction or prompt within the input text to manipulate the normal behavior of LLMs
		\item Prompt Engineering: Designing and crafting effective prompts or instructions for generating desired outputs from an LLM
	\end{itemize}
\end{itemize}


\newpage
\section{Solution Development}
\begin{itemize}
	\item From Prompt Engineering to RAG:
	\begin{itemize}
		\item Prompt: Input or query given to a LLM to elicit a specific response
		\item Prompt engineering: Practice of designing and refining prompts to optimize the responses generated by an AI model
		\item Prompt components: Instruction, context, input/question, output type/format
		\item Prompt techniques:
		\begin{itemize}
			\item Zero-shot: Prompt that generates text or performs a task without providing any examples or additional training specified to that task
			\item Few-shot: Prompt provides with a few input-output examples to guide the model for generating the desired output
		\end{itemize}
		\item Prompt chaining: Multiple tasks are linked together, with the output of one prompt serving as the input for the next $\to$ allows for more complex tasks to be broken down into manageable steps
		\item Chain-of-thought (CoT) prompting: Enhances the reasoning capabilities of LLMs by guiding them to articulate their though processes step-by-step
	\end{itemize}
	\item Tips and tricks:
	\begin{itemize}
		\item Different models may require different prompts
		\item Provide examples and clues to guide model's response generation
		\item Different use cases may require different prompts
		\item Use delimiters to distinguish between instruction and context
		\item Ask the model to return structured output
		\item Ask the model not to hallucinate, not to assume, not to rush
	\end{itemize}
	\item RAG:
	\begin{itemize}
		\item Passing context as model inputs improves factual recall
		\item Retrieve data/documents relevant to a question/task, provide them as context to augment the prompts to an LLM to improve generation
		\item Components:
		\begin{itemize}
			\item Index \& embed: Embedding model creates vector representations of the documents and the user query
			\item Vector store: Specialized to store unstructured data indexed by vectors
			\item Retrieval: Search stored vectors using similarity search to retrieve relevant information
			\item Filtering \& Reranking: Process of selecting or ranking retrieved documents before passing as context
			\item Prompt augmentation: Prompt engineering workflow to enhance context via injection of data retrieved from the vector store
			\item Generation
		\end{itemize}
		\item Flow: 
		\begin{itemize}
			\item Documents are embedded
			\item User asks question and LLM converts query to embeddings
			\item Similarity search for embedded documents and embedded user query
			\item Similar documents are passed as context to LLM generation model
			\item Model augments and generates completion
		\end{itemize}
		\item Databricks:
		\begin{itemize}
			\item DLT
			\item Mosaic AI Model Serving (embeddings, foundation models, custom models)
			\item Mosaic AI Vector Search
			\item Unity Catalog's Model registry
		\end{itemize}
		\item Benefits: 
		\begin{itemize}
			\item Up-to-date and accurate responses,
			\item Reducing inaccurate responses or hallucinations
			\item Domain-specific contextualization
			\item Efficiency and cost-effectiveness
		\end{itemize}
	\end{itemize}
	\item Demo:
	\begin{itemize}
		\item 
	\end{itemize}
\end{itemize}


\newpage
\section{Application Development}


\newpage
\section{Application Evaluation and Governance}


\newpage
\section{Application Deployment and Monitoring}

\end{document}