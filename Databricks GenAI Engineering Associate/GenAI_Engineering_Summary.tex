
\documentclass[11pt]{scrartcl}
\usepackage{graphicx}
\usepackage{amsmath, amssymb, textcomp}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{ucs}
\usepackage[T1]{fontenc}
\usepackage{subcaption} 	
\usepackage{hyperref}
\usepackage{fullpage}	
\usepackage{color}
\usepackage{enumitem}
\setlist[itemize]{noitemsep, topsep=1pt}
\usepackage[font=footnotesize]{caption}
\usepackage{draftwatermark}
\SetWatermarkText{Martin Fan√©}
\SetWatermarkScale{3}
\SetWatermarkColor[gray]{0.99}
\usepackage[numindex,nottoc,section]{} 


\def\Reg{\textsuperscript{\textregistered}}
\def\TM{\textsuperscript{\texttrademark}}
\def\CR{\textsuperscript{\textcopyright}}	


\usepackage{listings, xcolor}
\lstset
{	
	%	language     = Docker,
	basicstyle   = \ttfamily,
	keywordstyle = \color{blue}\ttfamily,
	stringstyle  = \color{orange}\ttfamily,
	commentstyle = \color{green}\ttfamily,
	morecomment  = [l][\color{teal}]{\#},
	%	backgroundcolor = \color{lightgray}\ttfamily,
	breaklines=true,
	tabsize=3,
	%	numbers=left,
	numberstyle=\color{black},
	numbersep=5pt, 
	frame=single
}

\begin{document}
\tableofcontents

\newpage
\section*{Fundamentals}
\begin{itemize}
	\item LLM: Model trained on massive datasets to achieve advanced language processing capabilities
	\item Foundation Models: Large ML model trained on vast amount of data \& fine-tuned for more specific language understanding and generation tasks 
\end{itemize}

\subsubsection*{Components}
\begin{itemize}
	\item Encoder: Converts text input into tokens (embeddings)
	\item Decoder: Converts generated output tokens back into meaningful words
	\item Transformer: Train the token embeddings
\end{itemize}

\subsubsection*{Databricks AI}
\begin{itemize}
	\item GenAI (Custom models, model serving, RAG)
	\item End-to-end AI (MLOps with MLFlow, AutoML, Monitoring, Governance)
	\item Databricks + MosaicML:
	\begin{itemize}
		\item Rapid democratization of model capabilities
		\item Making GenAI models work for enterprises
		\item Unifying AI and data stack
		\item Advantages: Customize models, secure environment, competitive
	\end{itemize}
\end{itemize}

\subsubsection*{Legal and Ethical Considerations}
\begin{itemize}
	\item Prompt Injection: Inserting a specific instruction or prompt within the input text to manipulate the normal behavior of LLMs
	\item Prompt Engineering: Designing and crafting effective prompts or instructions for generating desired outputs from an LLM
\end{itemize}


\newpage
\section{Solution Development}
\subsubsection*{From Prompt Engineering to RAG}
\begin{itemize}
	\item Prompt: Input or query given to a LLM to elicit a specific response
	\item Prompt engineering: Practice of designing and refining prompts to optimize the responses generated by an AI model
	\item Prompt components: Instruction, context, input/question, output type/format
	\item Prompt techniques:
	\begin{itemize}
		\item Zero-shot: Prompt that generates text or performs a task without providing any examples or additional training specified to that task
		\item Few-shot: Prompt provides with a few input-output examples to guide the model for generating the desired output
	\end{itemize}
	\item Prompt chaining: Multiple tasks are linked together, with the output of one prompt serving as the input for the next $\to$ allows for more complex tasks to be broken down into manageable steps
	\item Chain-of-thought (CoT) prompting: Enhances the reasoning capabilities of LLMs by guiding them to articulate their though processes step-by-step
\end{itemize}

\subsubsection*{Tips and tricks}
\begin{itemize}
	\item Different models may require different prompts
	\item Provide examples and clues to guide model's response generation
	\item Different use cases may require different prompts
	\item Use delimiters to distinguish between instruction and context
	\item Ask the model to return structured output
	\item Ask the model not to hallucinate, not to assume, not to rush
\end{itemize}

\subsubsection*{RAG}
\begin{itemize}
	\item Passing context as model inputs improves factual recall
	\item Retrieve data/documents relevant to a question/task, provide them as context to augment the prompts to an LLM to improve generation
	\item Components:
	\begin{itemize}
		\item Index \& embed: Embedding model creates vector representations of the documents and the user query
		\item Vector store: Specialized to store unstructured data indexed by vectors
		\item Retrieval: Search stored vectors using similarity search to retrieve relevant information
		\item Filtering \& Reranking: Process of selecting or ranking retrieved documents before passing as context
		\item Prompt augmentation: Prompt engineering workflow to enhance context via injection of data retrieved from the vector store
		\item Generation
	\end{itemize}
	\item Flow: 
	\begin{itemize}
		\item Documents are embedded
		\item User asks question and LLM converts query to embeddings
		\item Similarity search for embedded documents and embedded user query
		\item Similar documents are passed as context to LLM generation model
		\item Model augments and generates completion
	\end{itemize}
	\item Databricks:
	\begin{itemize}
		\item DLT
		\item Mosaic AI Model Serving (embeddings, foundation models, custom models)
		\item Mosaic AI Vector Search
		\item Unity Catalog's Model registry
	\end{itemize}
	\item Benefits: 
	\begin{itemize}
		\item Up-to-date and accurate responses,
		\item Reducing inaccurate responses or hallucinations
		\item Domain-specific contextualization
		\item Efficiency and cost-effectiveness
	\end{itemize}
\end{itemize}

\subsubsection*{Preparing Data for RAG Solution}
\begin{itemize}
	\item Issues:
	\begin{itemize}
		\item Poor quality model output
		\item Lost in the middle paradigm
		\item Inefficient retrieval
		\item Wring embedding model
	\end{itemize}
	\item Data prep process:
	\begin{itemize}
		\item Ingestion $\to$ Data storage $\to$ Chunking $\to$ Persist in vector store
		\item Delta Lake for storing data and tables in the Databricks DI Platform
		\item Unity Catalog for governance, discovery, access control for RAG applications
		\item Document extraction: Split documents into chunks, embed chunks with a model, store them in a vector store
	\end{itemize}
	\item Chunking data:
	\begin{itemize}
		\item Context-aware chunking: By sentence/paragraph/section, leverage special punctuation, include metadata/tags/titles
		\item Fixed-size chunking: Divide by a specific number of tokens, simple and computationally cheap method
		\item Chunking overlap defines the amount of overlap between consecutive chunks ensuring that no contextual information is lost
		\item Window summarization: Each chunk includes a windowed summary of previous chunks
	\end{itemize}
	\item Data preparation in Databricks:
	\begin{itemize}
		\item Ingestion: Tables and Volumes $\to$ Files \& metadata
		\item Document processing (parsing, cleaning, chunking, featurization): Workflows, DLT, Notebooks $\to$ Chunks \& features
		\item Embedding: Workflows, DLT, Notebooks $\to$ Chunks, vectors \& features
		\item Storage: Delta Tables $\to$ Automatic Sync
		\item Vector DB: Vector Search
	\end{itemize}
\end{itemize}

\subsubsection*{Vector Search}
\begin{itemize}
	\item Vector DB:
	\begin{itemize}
		\item Store and retrieve high dimensional vectors such as embeddings
		\item In RAG architecture, contextual information is stored in vectors
		\item Specialized and fully-fledged DB for unstructured data
		\item Speed up query search for closest vector
		\item Use cases: RAG, recommendation engines, similarity search
	\end{itemize}
	\item Vector similarity: 
	\begin{itemize}
		\item Distance metrics: L2 (Euclidian), Manhattan distance (L1)
		\item Similarity metrics: Cosine similarity
	\end{itemize}
	\item Vector search strategies: KNN, Approximate NN (ANN), Hierarchical Navigable Small Words (HNSW)
	\item Libraries create vector indices, plugins provide architectural enhancements
	\item Reranking:
	\begin{itemize}
		\item Prioritize documents most relevant to user's query
		\item Initial retrieval $\to$ Not all documents are equally important
		\item Reranker: Reorder documents based on the relevance scores $\to$ Place most relevant documents at the top of the list
		\item Adjusts initial ranking of retrieved documents to enhance precision and relevance
		\item Supports deeper semantic understanding
		\item Benefits: Improve accuracy, reduce hallucinations
		\item Challenges: LLM must be called repeatedly, increasing cost and latency, adds complexity to RAG pipeline
	\end{itemize}
	\item Mosaic AI Vector Search:
	\begin{itemize}
		\item Stores vectors and metadata, Integrated with Lakehouse, supports Access Control Level (ACL) using Unity Catalog
		\item Methods:
		\begin{itemize}
			\item Delta Sync API with managed embeddings: automatic sync, fully managed embeddings
			\item Delta Sync API with self-managed embeddings: automatic sync, self-managed embeddings
			\item Direct access CRUD API: manual sync via API, self-managed embeddings
		\end{itemize}
	\end{itemize}
	\item Set up:
	\begin{itemize}
		\item Create a Vector Search Endpoint: Compute resource
		\item Create a Model Serving Endpoint: Foundation Models APIs, external or custom models
		\item Create a Vector Search Index: Created and auto-synced from Delta Table, indexes appear in and are governed by Unity Catalog
	\end{itemize}
\end{itemize}

\subsubsection*{Assembling and Evaluating a RAG Application}
\begin{itemize}
	\item RAG Application Workflow: Development $\to$ Expert/User testing $\to$ Offline evaluation $\to$ Production
	\item MLFlow
	\item Evaluating RAG Pipeline:
	\begin{itemize}
		\item Components to evaluate: Chunking, embedding model, vector store (retrieval, reranker), generator
		\item Context precision: Signal-to-noise ratio for retrieved context, based on query and context
		\item Context relevancy: Measure relevancy of retrieved context, based on both the query and the context
		\item Context recall: Measures the extent to which all relevant entities and information are retrieved and mentioned in the context provided, based on ground truth and context
		\item Faithfulness: Measures the factual accuracy of generated answer in relation to provided context, based on responses and retrieved context
		\item Answer relevancy: Assess how applicable the generated response is to the user's query, based on the alignment of the response with the user's intent or query specifics
		\item Answer correctness: Measures accuracy of generated answer when compared to the ground truth, based on ground truth and response, encompasses both semantic and factual similarity with ground truth
	\end{itemize}
	\item MLFlow evaluation:
	\begin{itemize}
		\item Batch comparisons
		\item Rapid and scalable experimentation
		\item Cost effective
	\end{itemize}
\end{itemize}



\newpage
\section{Application Development}
\subsubsection*{Foundations of Compound AI Systems}
\begin{itemize}
	\item Compound AI System: AI System that has multiple interacting components
	\item Systems: Prompt engineering, RAG, Agent-based chain, orchestration chain etc.
	\item Prompts have multiple intents $\to$ Consists of one to many tasks, e.g., translate, summarize, analyze sentiment, and classify
\end{itemize}
\begin{itemize}
	\item Designing Compound AI Systems:
	\begin{itemize}
		\item Approach: Analysis, design, development, production, monitoring
		\item Identify intents, tools, build the chain
	\end{itemize}
\end{itemize} 


\subsubsection*{Building Multi-Stage Reasoning Chains}
\begin{itemize}
	\item Map concepts with technical terms
	\item Composition frameworks help to manage multi-stage reasoning systems
	\item LangChain main components:
	\begin{itemize}
		\item Prompt: Structured text to communicate a specific task/query to an LLM
		\item Chain: Sequence of automated actions or components
		\item Retriever: Interface returning relevant documents
		\item Tool: Functionality/resource that an agent can activate to perform a specific task
	\end{itemize}
	\item LlamaIndex: Framework that enhances capabilities of LLMs by structuring and indexing data
	\item Haystack: Python framework for building applications with LLMs, focusing on document retrieval, text generation, and summarization
	\item Databricks products for Building Multi-stage Reasoning Systems:
	\begin{itemize}
		\item Foundation Model API $\to$ Access and query open GenAI models
		\item DBRX:
		\begin{itemize}
			\item DBRX Base: pre-trained model, functions like smart auto-complete, useful for further fine-tuning
			\item DBRX Instruct: fine-tuned model, answers questions and follows instructions
		\end{itemize}
		\item Vector Search:
		\begin{itemize}
			\item Stores vector representation of data and metadata
			\item Integrated with Lakehouse
			\item API for real-time similarity search
		\end{itemize}
	\end{itemize}
\end{itemize}


\subsubsection*{Agents and Cognitive Architectures}
\begin{itemize}
	\item Agent: 
	\begin{itemize}
		\item Application that can execute complex tasks by using a LM to define a sequence of actions to take
		\item Compound AI systems: Hard-coded calls to external tools and services
		\item Agents replace hard-coded sequence of actions with a query-dependent sequences chosen dynamically by LLMs
	\end{itemize}
	\item Non-agentic workflow: LLM generates an answer based on actions defined
	\item Agentic workflow: Agent does research, writes first draft, another model checks the draft (less determined)
	\item Workflow:
	\begin{itemize}
		\item Process tasks
		\item Collect data
		\item Data analysis
		\item Output generation
	\end{itemize}
	\item Agent Reasoning: Cognitive process by which AI agents analyze information, draw logical conclusions, and make decisions autonomously
	\item Design patterns:
	\begin{itemize}
		\item ReAct (Reason + Act):
		\begin{itemize}
			\item Enables models to verbal reasoning traces and actions
			\item Main states:
			\begin{itemize}
				\item Thought: Reflect on the problem given and previous actions taken
				\item Act: Choose correct tool and input format to use
				\item Observe: Evaluate the result of the action and generate next thought
			\end{itemize}
		\end{itemize}
		\item Tool use / function calling: Research tools, document retrieval, coding, image
		\item Planning: Agents must be able to dynamically adjust their goals and plans based on changing conditions
		\item Multi-agent collaboration
	\end{itemize}
	\item Multi-Modal AI: Models with inputs/outputs that include data types beyond text (image, audio, video)
\end{itemize}

\newpage
\section{Application Evaluation and Governance}
\subsubsection*{Securing and Governing GenAI Applications}
\begin{itemize}
	\item Data and AI Security Framework (DASF)
	Databricks as Security:
	\begin{itemize}
		\item Algorithm: Model Serving, Lakehouse Monitoring
		\item Evaluation: MLflow, Lakehouse Monitoring
		\item Model Management: MLflow, Unity Catalog
		\item Catalog: Unity Catalog
		\item Operations: Asset Bundles, CLI, Secrets
		\item Platform: Cloud architecture, Serverless
	\end{itemize}
	\item Key Security Tooling: Unity Catalog, Mosaic AI
\end{itemize}


\subsubsection*{Evaluation Techniques}
\begin{itemize}
	\item Metrics: 
	\begin{itemize}
		\item Human feedback, LLM-as-a-judge
		\item Perplexity $\to$ Low Perplexity = High confidence
		\item Toxicity $\to$ How harmful is the model?
	\end{itemize}
	\item Benchmarking: Compare models against standard evaluation datasets
	\item LLM-as-a-judge
\end{itemize}

\subsubsection*{End-to-End Application Evaluation}


\newpage
\section{Application Deployment and Monitoring}

\end{document}