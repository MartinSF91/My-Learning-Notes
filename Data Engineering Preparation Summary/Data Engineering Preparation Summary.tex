
\documentclass[11pt]{scrartcl}
\usepackage{graphicx}
\usepackage{amsmath, amssymb, textcomp}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{ucs}
\usepackage[T1]{fontenc}
\usepackage{subcaption} 		% Subfigures
\usepackage{hyperref}
\usepackage{fullpage}	
\usepackage{color}
\usepackage{enumitem}
\setlist[itemize]{noitemsep, topsep=1pt}
\usepackage[font=footnotesize]{caption}
\usepackage{draftwatermark}
\SetWatermarkText{Martin Fané}
\SetWatermarkScale{3}
\SetWatermarkColor[gray]{0.99}
\usepackage[numindex,nottoc,section]{} 


\begin{document}
\subsubsection*{Data Lake}
\begin{itemize}
	\item Ein Data Lake ist ein zentrales Repository, das strukturierte, semi-strukturierte und unstrukturierte Rohdaten in großem Umfang speichert
	\item Eignet sich besonders für Big-Data-Analysen und Machine Learning, erfordert aber zusätzliche Verarbeitung zur Datennutzung
\end{itemize}

\subsubsection*{Data Warehouse}
\begin{itemize}
	\item Ein Data Warehouse ist ein zentrales, strukturiertes System zur langfristigen Speicherung und Analyse großer Datenmengen aus verschiedenen Quellen
	\item Optimiert für komplexe Abfragen und Business-Intelligence-Anwendungen
\end{itemize}

\subsubsection*{Cloud Data Warehouse}
\begin{itemize}
	\item Ein Cloud Data Warehouse ist ein Data Warehouse, das in der Cloud gehostet wird und dadurch flexibel skalierbar und wartungsarm ist. Beispiele sind Snowflake, Google BigQuery oder Amazon Redshift
\end{itemize}

\subsubsection*{Data Lakehouse}
\begin{itemize}
	\item Datenarchitektur, die die Flexibilität und Skalierbarkeit eines Data Lakes mit den strukturierten Datenmanagement- und Analysefunktionen eines Data Warehouses kombiniert
	\item Ermöglicht sowohl explorative Datenanalyse als auch strukturierte BI-Abfragen auf einer gemeinsamen Plattform
	\item ACID Transaktionen:
	\begin{itemize}
		\item Atomicity: Transaktion wird entweder vollständig ausgeführt oder gar nicht – es gibt keine halbfertigen Zustände
		\item Consistency: Transaktion überführt die Datenbank von einem konsistenten Zustand in einen anderen, wobei alle definierten Regeln (z.B. Integritätsbedingungen) eingehalten werden
		\item Isolation: Gleichzeitige Transaktionen beeinflussen sich nicht gegenseitig – jede Transaktion läuft so, als wäre sie allein im System
		\item Durability: Sobald eine Transaktion abgeschlossen ist, bleiben ihre Änderungen dauerhaft gespeichert, selbst bei Systemausfällen
	\end{itemize}
\end{itemize}	

\subsubsection*{Data Mart}
\begin{itemize}
	\item Spezialisierte Teilmenge eines Data Warehouses, die auf die Anforderungen eines bestimmten Fachbereichs zugeschnitten ist. Es verbessert die Performance und Übersichtlichkeit für gezielte Analysen
\end{itemize}

\subsubsection*{Data Mesh}
\begin{itemize}
	\item Dezentraler Ansatz zur Datenarchitektur, bei dem einzelne Teams für ihre eigenen Datenprodukte verantwortlich sind ("Data as a Product")
	\item Fördert Skalierbarkeit und Eigenverantwortung durch domänenorientierte Datenverwaltung
\end{itemize}

\subsubsection*{Datenprodukt}
\begin{itemize}
	\item Wiederverwendbares, klar definiertes Datenartefakt (Datensatz), das für einen konkreten Anwendungsfall erstellt wird und von anderen leicht konsumiert werden kann
\end{itemize}

\subsubsection*{ETL}
\begin{itemize}
	\item Prozess, bei dem Daten aus verschiedenen Quellen extrahiert, in ein geeignetes Format transformiert und schließlich in ein Zielsystem wie ein Data Warehouse geladen werden
	\item Dient dazu, Rohdaten für Analysen nutzbar zu machen, indem sie bereinigt, vereinheitlicht und angereichert werden
	\item Moderne ETL-Prozesse können auch in Echtzeit (Streaming-ETL) oder als ELT-Variante ablaufen, bei der die Transformation nach dem Laden erfolgt
\end{itemize}

\subsubsection*{ETL Pipeline}
\begin{itemize}
	\item Extraktion $ \to $ ADF (Extrahieren von Daten aus verschiedenen Quellen)
	\item Transform $ \to $ Databricks und Spark (Verarbeiten, bereinigen und anreichern der Daten in skalierbaren Umgebungen)
	\item Load $ \to $ Snowflake, BigQuery (Laden der transformierten Daten in ein Cloud DWH)
	\item Orchestrierung $ \to $ Apache Airflow (Steuerung und Automatisierung des Ablaufs der gesamten ETL-Prozesse)
	\item Montoring $ \to $ Grafana, Azure Monitor (Überwachung von Performance, Ausfällen und Fehlern der Pipeline)
	\item Data Quality \& Testing $ \to $ Great Expectations (Validieren der Datenqualität durch automatisierte Tests und Regeln)
\end{itemize}


\subsubsection*{Pipeline Montoring}
\begin{itemize}
	\item Überwachung von Datenpipelines, Systemen und Prozessen, um sicherzustellen, dass Daten zuverlässig, korrekt und zielgerecht verarbeitet werden
	\item Fehlererkennung:
	\begin{itemize}
		\item Erkennen von fehlgeschlagenen ETL-Jobs
		\item Identifikation von Datenanomalien
		\item Monitoring von Datenlatenzen
	\end{itemize}
	\item Performance Überwachung:
	\begin{itemize}
		\item Laufzeit von Pipelines
		\item Ressourcenverbrauch
		\item Datenvolumen und Verarbeitungsraten
	\end{itemize}
	\item Sicherstellen der Datenqualität:
	\begin{itemize}
		\item Validierung von Daten gegen Regeln
		\item Schema Überwachung
	\end{itemize}
	\item Transparenz und Reporting:
	\begin{itemize}
		\item Dashboarding von Metriken (Grafana, Power BI)
		\item Alerting bei Schwellenwertüberschreitung
	\end{itemize}
	\item Beispiele:
	\begin{itemize}
		\item Apache Airflow Monitoring: Überwacht den Status von DAGs, ob Tasks fehlschlagen oder hängenbleiben
		\item Snowflake oder BigQuery Monitoring: Überwachung von Query-Ausführungszeiten und -Kosten
		\item Streaming Monitoring: Sicherstellen, dass Messages rechtzeitig und vollständig verarbeitet werden
	\end{itemize}
\end{itemize}


\subsubsection*{Datenmanagement}
\begin{itemize}
	\item Übergeordnete Prozess der Erfassung, Speicherung, Organisation, Pflege und Nutzung von Daten in einem Unternehmen
	\item Umfasst verschiedene Disziplinen wie Data Governance, Datenintegration, Datenqualität, Metadatenmanagement und Archivierung, um den wertschöpfenden Einsatz von Daten sicherzustellen
	\item Datenintegrität: Zusammenführen und Vereinheitlichung von Daten aus verschiedenen Quellen 
\end{itemize}

\subsubsection*{Data Governance}
\begin{itemize}
	\item Bezeichnet den Rahmen aus Richtlinien, Prozessen und Verantwortlichkeiten, der sicherstellt, dass Daten im Unternehmen korrekt, sicher, einheitlich und regelkonform verwaltet werden
	\item Umfasst Themen wie Datenqualität, Datenschutz, Zugriffsrechte, Compliance und Datenverantwortung, um Vertrauen und Kontrolle über Daten zu gewährleisten
\end{itemize}


\subsubsection*{Datenqualität}
\begin{itemize}
	\item Vollständigkeit $ \to $ Sind alle erforderlichen Datenfelder vorhanden und ausgefüllt?
	\item Korrektheit $ \to $ Entsprechen die Daten den realen, erwarteten Werten (z.B. stimmen Postleitzahlen mit Städten überein)?
	\item Konsistenz $ \to $ Sind die Daten in verschiedenen Systemen oder Tabellen widerspruchsfrei (z.B. gleicher Kundennamen in allen Datensätzen)?
	\item Aktualität $ \to $ Sind die Daten aktuell bzw. zeitgerecht verarbeitet (z.B. keine veralteten Transaktionen im Reporting)?
	\item Eindeutigkeit $ \to $ Gibt es doppelte Datensätze, wo es keine geben sollte (z.B. doppelte Kunden-IDs)?
	\item Validität $ \to $ Entsprechen die Daten den erwarteten Formaten oder Regeln (z.B. E-Mail-Adressen im gültigen Format, Zahlen in numerischen Feldern)?
\end{itemize}

\subsubsection*{CI/CD}
\begin{itemize}
	\item CI: Automatisches Testen und Bauen bei jeder Codeänderung
	\item CDelivery: Automatisches Ausliefern in eine Staging-Umgebung
	\item CDeployment: Automatisches Ausliefern bis in die Produktion (ohne manuelle Eingriffe)	
\end{itemize}

\subsubsection*{Data Analytics Konzepte}
\begin{itemize}
	\item EDA
	\item Deskriptive, diagnostische, prädiktive und präskriptive Analysen
	\item KPI-Tracking: Überwachung von Leistungskennzahlen
\end{itemize}

\subsubsection*{Dokumentation}
\begin{itemize}
	\item Datenquellen:
	\begin{itemize}
		\item Herkunft (Systeme, APIs, Dateien)
		\item Zugriffsmethoden und -rechte
		\item Aktualisierungsfrequenz
	\end{itemize}
	\item ETL-/ELT-Prozesse:
	\begin{itemize}
		\item Datenflüsse und Pipeline-Übersichten
		\item Verwendete Transformationslogik (SQL, dbt, Spark etc.)
		\item Abhängigkeiten zwischen Schritten
	\end{itemize}
	\item Datenmodelle und Schemata:
	\begin{itemize}
		\item Tabellen, Views, Spaltenbeschreibungen
		\item Beziehungen zwischen Entitäten (z.B. ER-Diagramme)
		\item Versionierung von Modellen
	\end{itemize}
	\item Datenqualitätsregeln:
	\begin{itemize}
		\item Validierungschecks
		\item Testdefinitionen (z.B. dbt tests, Great Expectations)
	\end{itemize}
	\item Zugriffs- und Sicherheitsrichtlinien:
	\begin{itemize}
		\item Rollen und Berechtigungen
		\item Datenklassifizierung (z.B. PII, öffentlich, vertraulich)
	\end{itemize}
	\item Monitoring \& Alerting:
	\begin{itemize}
		\item Überwachte Metriken
		\item Fehlerbehandlung und Wiederanläufe
		\item SLAs und SLOs
	\end{itemize}
	\item Code- und Tool-Dokumentation:
	\begin{itemize}
		\item Repos, Technologien, Abhängigkeiten
		\item Setup-Anleitungen und Deployment-Prozesse
	\end{itemize}
\end{itemize}


































\end{document}