
\documentclass[11pt]{scrartcl}
\usepackage{amsmath, amssymb, textcomp}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage{ucs}
\usepackage[T1]{fontenc}
\usepackage{fullpage}	
\usepackage{enumitem}
\setlist[itemize]{noitemsep, topsep=1pt}
\usepackage{draftwatermark}
\SetWatermarkText{Martin Fané}
\SetWatermarkScale{3}
\SetWatermarkColor[gray]{0.97}
\renewcommand\familydefault{\sfdefault}



\begin{document}
\tableofcontents 

\newpage
\section{Data Engineering Storage Abstractions}
\subsection{Data Lake}
\begin{itemize}
	\item Ein Data Lake ist ein zentrales Repository, das strukturierte, semi-strukturierte und unstrukturierte Rohdaten in großem Umfang speichert
	\item Eignet sich besonders für Big-Data-Analysen und Machine Learning, erfordert aber zusätzliche Verarbeitung zur Datennutzung
	\item Beispiel: Amazon S3
\end{itemize}


\subsection{Data Warehouse}
\begin{itemize}
	\item Zentrales, strukturiertes System zur langfristigen Speicherung und Analyse großer Datenmengen aus verschiedenen Quellen
	\item Optimiert für komplexe Abfragen und Business-Intelligence-Anwendungen
	\item Beispiel: MS SQL Server
\end{itemize}


\subsection{Cloud Data Warehouse}
\begin{itemize}
	\item Ein Cloud Data Warehouse ist ein Data Warehouse, das in der Cloud gehostet wird (keine eigene Infrastruktur notwendig) und dadurch flexibel skalierbar und wartungsarm ist
	\item Beispiele: Snowflake, Google BigQuery oder Amazon Redshift
\end{itemize}


\subsection{Data Lakehouse}
\begin{itemize}
	\item Datenarchitektur, die die Flexibilität und Skalierbarkeit eines Data Lakes mit den strukturierten Datenmanagement- und Analysefunktionen eines Data Warehouses kombiniert
	\item Ermöglicht sowohl explorative Datenanalyse als auch strukturierte BI-Abfragen auf einer gemeinsamen Plattform
	\item ACID Transaktionen:
	\begin{itemize}
		\item Atomicity: Transaktion wird entweder vollständig ausgeführt oder gar nicht – es gibt keine halbfertigen Zustände
		\item Consistency: Transaktion überführt die Datenbank von einem konsistenten Zustand in einen anderen, wobei alle definierten Regeln (z.B. Integritätsbedingungen) eingehalten werden
		\item Isolation: Gleichzeitige Transaktionen beeinflussen sich nicht gegenseitig – jede Transaktion läuft so, als wäre sie allein im System
		\item Durability: Sobald eine Transaktion abgeschlossen ist, bleiben ihre Änderungen dauerhaft gespeichert, selbst bei Systemausfällen
	\end{itemize}
	\item Beispiel: Databricks
\end{itemize}


\subsection{Delta Lake}
\begin{itemize}
	\item Open-Source-Storage-Schicht, die auf Data Lakes (z.B. in S3 oder Azure Blob Storage) aufsetzt und Funktionen wie ACID-Transaktionen, Schema-Management, Zeitreisen (Versionierung) und verlässliche Upserts/Merges bietet
	\item Damit macht Delta Lake rohe Data Lakes zuverlässiger, konsistenter und besser für Analytics und Machine Learning geeignet – indem es typische Probleme von Data Lakes (wie inkonsistente Daten oder fehlende Transaktionen) löst. Es wird oft mit Apache Spark und Databricks genutzt
	\item Schema Enforcement:
	\begin{itemize}
		\item Stellt sicher, dass neue Daten, die in eine Delta-Tabelle geschrieben werden, dem bestehenden Tabellenschema entsprechen
		\item Dadurch werden fehlerhafte oder inkonsistente Daten (z.B. falsche Datentypen oder fehlende Spalten) automatisch abgelehnt, um Datenqualität und Konsistenz zu gewährleisten
	\end{itemize}
	\item Time Travel:
	\begin{itemize}
		\item Ermöglicht den Zugriff auf frühere Versionen einer Tabelle, indem man einen bestimmten Zeitpunkt oder Versionsnummer angibt
		\item Historische Daten analysieren, vergleichen, Wiederherstellungen durchführen oder versehentliche Änderungen rückgängig machen
	\end{itemize}
	\item Beispiel: Delta Lake on Databricks
\end{itemize}	


\subsection{Data Mart}
\begin{itemize}
	\item Spezialisierte Teilmenge eines Data Warehouses, die auf die Anforderungen eines bestimmten Fachbereichs zugeschnitten ist
	\item Verbessert die Performance und Übersichtlichkeit für gezielte Analysen
	\item Beispiel: Amazon Redshift Data Mart
\end{itemize}


\subsection{Data Mesh}
\begin{itemize}
	\item Dezentraler Ansatz zur Datenarchitektur, bei dem einzelne Teams für ihre eigenen Datenprodukte verantwortlich sind ("Data as a Product")
	\item Fördert Skalierbarkeit und Eigenverantwortung durch domänenorientierte Datenverwaltung
\end{itemize}


\subsection{Data Vault}
\begin{itemize}
	\item Modellgetriebene Methode zur Gestaltung von Data Warehouses, die speziell für historisierbare, skalierbare und auditierbare Datenarchitekturen entwickelt wurde
	\item Sie trennt Daten in drei Hauptkomponenten:
	\begin{itemize}
		\item Hubs (Schlüsselentitäten, z.B. Kunden-ID),
		\item Links (Beziehungen zwischen Hubs, z.B. Kunden $ \leftrightarrow $ Bestellungen)
		\item Satellites (beschreibende, historisierte Daten mit Zeitstempel, z.B. Kundenname, Adresse).
	\end{itemize}
	\item Eignet sich besonders für agile, stark wachsende Datenumgebungen mit hohem Bedarf an Nachvollziehbarkeit und Flexibilität.
\end{itemize}


\newpage
\section{Datenmanagement}
\begin{itemize}
\item Übergeordnete Prozess der Erfassung, Speicherung, Organisation, Pflege und Nutzung von Daten in einem Unternehmen
\item Umfasst verschiedene Disziplinen wie Data Governance, Datenintegration, Datenqualität, Metadatenmanagement und Archivierung, um den wertschöpfenden Einsatz von Daten sicherzustellen
\item Datenintegrität: Zusammenführen und Vereinheitlichung von Daten aus verschiedenen Quellen 
\end{itemize}


\subsection{Data Governance}
\begin{itemize}
\item Bezeichnet den Rahmen aus Richtlinien, Prozessen und Verantwortlichkeiten, der sicherstellt, dass Daten im Unternehmen korrekt, sicher, einheitlich und regelkonform verwaltet werden
\item Umfasst Themen wie Datenqualität, Datenschutz, Zugriffsrechte, Compliance und Datenverantwortung, um Vertrauen und Kontrolle über Daten zu gewährleisten
\end{itemize}


\subsection{Datenqualität}
\begin{itemize}
\item Vollständigkeit $ \to $ Sind alle erforderlichen Datenfelder vorhanden und ausgefüllt?
\item Korrektheit $ \to $ Entsprechen die Daten den realen, erwarteten Werten (z.B. stimmen Postleitzahlen mit Städten überein)
\item Konsistenz $ \to $ Sind die Daten in verschiedenen Systemen oder Tabellen widerspruchsfrei (z.B. gleicher Kundennamen in allen Datensätzen)
\item Aktualität $ \to $ Sind die Daten aktuell bzw. zeitgerecht verarbeitet (z.B. keine veralteten Transaktionen im Reporting)
\item Eindeutigkeit $ \to $ Gibt es doppelte Datensätze, wo es keine geben sollte (z.B. doppelte Kunden-IDs)
\item Validität $ \to $ Entsprechen die Daten den erwarteten Formaten oder Regeln (z.B. E-Mail-Adressen im gültigen Format, Zahlen in numerischen Feldern)
\end{itemize}


\subsection{Datenintegration}
\begin{itemize}
	\item Prozess, bei dem Daten aus verschiedenen Quellen zusammengeführt, vereinheitlicht und für eine zentrale Nutzung bereitgestellt werden
	\item Ziel ist es, konsistente, vollständige und aktuelle Informationen für Analysen, Berichte oder operative Systeme bereitzustellen
\end{itemize}


\subsection{Daten Lifecycle}
\begin{itemize}
	\item Beschreibt die verschiedenen Phasen, die Daten während ihrer Existenz durchlaufen – von der Erfassung über Speicherung, Verarbeitung, Nutzung und Weitergabe bis hin zur Archivierung oder Löschung
	\item Hilft, Daten systematisch zu verwalten, deren Qualität zu sichern und rechtliche sowie sicherheitsrelevante Anforderungen zu erfüllen	
\end{itemize}


\subsection{Data Catalog}
\begin{itemize}
	\item Zentrales Verzeichnis, das Metadaten über Datenbestände innerhalb eines Unternehmens organisiert, beschreibt und auffindbar macht
	\item Hilft Nutzern dabei, Datenquellen schnell zu finden, deren Struktur und Bedeutung zu verstehen und die Daten effizient zu nutzen – oft unterstützt durch Suchfunktionen, Tags und Datenklassifizierung
\end{itemize}


\subsection{Unity Catalog}
\begin{itemize}
	\item Einheitliches Datenverwaltungssystem von Databricks, das Zugriffskontrolle, Daten-Governance und Katalogisierung für alle Daten-Assets über verschiedene Workloads hinweg (z.B. SQL, Python, BI-Tools) ermöglicht
	\item Bietet zentrale Verwaltung von Benutzerrechten, Datenklassifikation und Lineage (Herkunftsnachverfolgung) über mehrere Workspaces und Clouds hinweg – für mehr Sicherheit, Transparenz und Zusammenarbeit in Data- und ML-Projekten
\end{itemize}


\subsection{Feature Store}
\begin{itemize}
\item Zentrale Plattform zur Speicherung, Verwaltung und Wiederverwendung von Merkmalen (Features), die für Machine-Learning-Modelle verwendet werden
\item Ermöglicht konsistente und effiziente Bereitstellung von Features sowohl für das Training als auch für die Echtzeit-Vorhersage in produktiven ML-Systemen
\end{itemize}

\newpage
\section{Daten}
\subsection{Datenprodukt}
\begin{itemize}
	\item Wiederverwendbares, klar definiertes Datenartefakt (Datensatz), das für einen konkreten Anwendungsfall erstellt wird und von anderen leicht konsumiert werden kann
\end{itemize}


\subsection{Data Lineage}
\begin{itemize}
	\item Beschreibt die Herkunft, den Weg und die Transformationen von Daten über verschiedene Systeme hinweg – vom Ursprung bis zur Nutzung, etwa in Berichten oder Analysen
	\item Hilft dabei, Datenflüsse nachvollziehbar zu machen, die Datenqualität zu sichern und regulatorische Anforderungen zu erfüllen
\end{itemize}


\subsection{Daten Versionierung}
\begin{itemize}
	\item Systematische Erfassen, Speichern und Verwalten verschiedener Zustände oder Versionen von Datensätzen im Zeitverlauf
	\item Dadurch können Änderungen nachvollzogen, frühere Datenzustände wiederhergestellt und Reproduzierbarkeit in Analysen und Machine-Learning-Modellen sichergestellt werden
\end{itemize}


\subsection{Datenmodellierung}
\begin{itemize}
	\item Prozess der strukturierten Darstellung von Daten und deren Beziehungen untereinander, meist als Diagramm oder Modell
	\item Ziel ist es, ein klares, logisches Gerüst für die Speicherung, Verwaltung und Nutzung von Daten zu schaffen – typischerweise in Form von konzeptionellen, logischen und physischen Datenmodellen
\end{itemize}


\subsubsection{Methoden}
\begin{itemize}
	\item Relationales Modell:
	\begin{itemize}
		\item Grundlage für relationale Datenbanken; Daten werden in Tabellen (Relationen) mit Zeilen und Spalten organisiert
		\item Beispiel: Tabelle Kunde mit Spalten wie KundenID, Name, Adresse.
	\end{itemize}
	\item Entity-Relationship-Modell:
	\begin{itemize}
		\item Visualisiert Entitäten und deren Beziehungen zur konzeptuellen Planung von Datenstrukturen
		\item Beispiel: Entitäten \textit{Kunde} und \textit{Bestellung} sind über eine Beziehung \textit{tätigt} verbunden
	\end{itemize}
	\item Star Schema:
	\begin{itemize}
		\item Häufig im Data-Warehouse-Bereich; eine zentrale Faktentabelle ist mit mehreren Dimensionstabellen verbunden
		\item Beispiel: Faktentabelle \textit{Umsatz} mit Verbindungen zu \textit{Produkt, Zeit, Kunde, Standort}
	\end{itemize}
\end{itemize}


\subsection{Datenformate}
\begin{itemize}
	\item Parquet:
	\begin{itemize}
		\item Spaltenbasiertes Speicherformat
		\item Komprimiert und effizient für Analyse-Workloads
	\end{itemize}
	\item CSV:
	\begin{itemize}
		\item Einfaches, textbasiertes Format
		\item Weit verbreitet, aber keine Schema- oder Typinformation
	\end{itemize}
	\item JSON:
	\begin{itemize}
		\item Textbasiert, semi-strukturiert
		\item Gut für hierarchische Daten, aber größer und langsamer als binäre Formate
	\end{itemize}
	\item Delta Table:
	\begin{itemize}
		\item Baut auf dem Apache Parquet Format auf und wird durch Delta Lake erweitert, um transaktionale Konsistenz, Schema-Evolution und Zeitreisen (Versionierung) in Data Lakes zu ermöglichen.
		\item Erlaubt ACID-Transaktionen auf großen Datenmengen, was insbesondere bei Big-Data-Analysen und Machine Learning für zuverlässige, reproduzierbare Datenpipelines sorgt
	\end{itemize}
	\item XML:
	\begin{itemize}
		\item Textbasiert, hierarchisch, aber oft sehr groß und weniger performant
	\end{itemize}
\end{itemize}


\subsection{Feature Engineering}
\begin{itemize}
	\item Skalierbare und automatisierte Aufbereitung von Merkmalen (Features) für Machine-Learning-Modelle innerhalb von Datenpipelines
	\item Data Engineers bauen dafür robuste, reproduzierbare Prozesse zur Berechnung, Speicherung und Bereitstellung von Features – z.B. über Feature Stores – und sorgen dafür, dass diese Merkmale konsistent, versioniert und performant für Training und Inferenz verfügbar sind
\end{itemize}


\newpage
\section{Daten Pipeline}
\subsection{ETL}
\begin{itemize}
	\item Prozess, bei dem Daten aus verschiedenen Quellen extrahiert, in ein geeignetes Format transformiert und schließlich in ein Zielsystem wie ein Data Warehouse geladen werden
	\item Dient dazu, Rohdaten für Analysen nutzbar zu machen, indem sie bereinigt, vereinheitlicht und angereichert werden
	\item Moderne ETL-Prozesse können auch in Echtzeit (Streaming-ETL) oder als ELT-Variante ablaufen, bei der die Transformation nach dem Laden erfolgt
\end{itemize}


\subsection{ETL Pipeline}
\begin{itemize}
	\item Extraktion $ \to $ ADF (Extrahieren von Daten aus verschiedenen Quellen)
	\item Transform $ \to $ Databricks und Spark (Verarbeiten, bereinigen und anreichern der Daten in skalierbaren Umgebungen)
	\item Load $ \to $ Snowflake, BigQuery (Laden der transformierten Daten in ein Cloud DWH)
	\item Orchestrierung $ \to $ Apache Airflow (Steuerung und Automatisierung des Ablaufs der gesamten ETL-Prozesse)
	\item Montoring $ \to $ Grafana, Azure Monitor (Überwachung von Performance, Ausfällen und Fehlern der Pipeline)
	\item Data Quality \& Testing $ \to $ Great Expectations (Validieren der Datenqualität durch automatisierte Tests und Regeln)
\end{itemize}


\subsection{Pipeline Montoring}
\begin{itemize}
	\item Überwachung von Datenpipelines, Systemen und Prozessen, um sicherzustellen, dass Daten zuverlässig, korrekt und zielgerecht verarbeitet werden
	\item Fehlererkennung:
	\begin{itemize}
		\item Erkennen von fehlgeschlagenen ETL-Jobs
		\item Identifikation von Datenanomalien
		\item Monitoring von Datenlatenzen
	\end{itemize}
	\item Performance Überwachung:
	\begin{itemize}
		\item Laufzeit von Pipelines
		\item Ressourcenverbrauch
		\item Datenvolumen und Verarbeitungsraten
	\end{itemize}
	\item Sicherstellen der Datenqualität:
	\begin{itemize}
		\item Validierung von Daten gegen Regeln
		\item Schema Überwachung
	\end{itemize}
	\item Transparenz und Reporting:
	\begin{itemize}
		\item Dashboarding von Metriken (Grafana, Power BI)
		\item Alerting bei Schwellenwertüberschreitung
	\end{itemize}
	\item Beispiele:
	\begin{itemize}
		\item Apache Airflow Monitoring: Überwacht den Status von DAGs, ob Tasks fehlschlagen oder hängenbleiben
		\item Snowflake oder BigQuery Monitoring: Überwachung von Query-Ausführungszeiten und -Kosten
		\item Streaming Monitoring: Sicherstellen, dass Messages rechtzeitig und vollständig verarbeitet werden
	\end{itemize}
\end{itemize}


\subsection{CI/CD}
\begin{itemize}
	\item CI: Automatisches Testen und Bauen bei jeder Codeänderung
	\item CDelivery: Automatisches Ausliefern in eine Staging-Umgebung
	\item CDeployment: Automatisches Ausliefern bis in die Produktion (ohne manuelle Eingriffe)	
\end{itemize}


\subsection{Orchestrierung}
\begin{itemize}
\item Zentrale Steuerung, Planung und Überwachung von Datenprozessen (wie ETL- oder ELT-Jobs), sodass sie in der richtigen Reihenfolge, zur richtigen Zeit und abhängig voneinander ausgeführt werden
\item Sorgt für Automatisierung, Fehlerbehandlung, Wiederholbarkeit und Effizienz komplexer Datenpipelines – typischerweise mit Tools wie Apache Airflow, Prefect oder Dagster
\end{itemize}


\subsection{Logging}
\begin{itemize}
\item Strukturiertes Erfassen und Speichern von Informationen über den Ablauf, Status und mögliche Fehler von Datenprozessen (z.B. ETL-Jobs, Pipelines, APIs)
\item Dient der Überwachung, Fehlerdiagnose, Auditierbarkeit und Performanceanalyse von Datenflüssen – oft zentral für Debugging und Betriebssicherheit
\end{itemize}


\subsection{Data Analytics Konzepte}
\begin{itemize}
	\item EDA
	\item Deskriptive, diagnostische, prädiktive und präskriptive Analysen
	\item KPI-Tracking: Überwachung von Leistungskennzahlen
\end{itemize}

\newpage
\section{Dokumentation}
\begin{itemize}
	\item Datenquellen:
	\begin{itemize}
		\item Herkunft (Systeme, APIs, Dateien)
		\item Zugriffsmethoden und -rechte
		\item Aktualisierungsfrequenz
	\end{itemize}
	\item ETL-/ELT-Prozesse:
	\begin{itemize}
		\item Datenflüsse und Pipeline-Übersichten
		\item Verwendete Transformationslogik (SQL, dbt, Spark etc.)
		\item Abhängigkeiten zwischen Schritten
	\end{itemize}
	\item Datenmodelle und Schemata:
	\begin{itemize}
		\item Tabellen, Views, Spaltenbeschreibungen
		\item Beziehungen zwischen Entitäten (z.B. ER-Diagramme)
		\item Versionierung von Modellen
	\end{itemize}
	\item Datenqualitätsregeln:
	\begin{itemize}
		\item Validierungschecks
		\item Testdefinitionen (z.B. dbt tests, Great Expectations)
	\end{itemize}
	\item Zugriffs- und Sicherheitsrichtlinien:
	\begin{itemize}
		\item Rollen und Berechtigungen
		\item Datenklassifizierung (z.B. PII, öffentlich, vertraulich)
	\end{itemize}
	\item Monitoring \& Alerting:
	\begin{itemize}
		\item Überwachte Metriken
		\item Fehlerbehandlung und Wiederanläufe
		\item SLAs und SLOs
	\end{itemize}
	\item Code- und Tool-Dokumentation:
	\begin{itemize}
		\item Repos, Technologien, Abhängigkeiten
		\item Setup-Anleitungen und Deployment-Prozesse
	\end{itemize}
\end{itemize}


\newpage
\section{Batch vs. Streaming Processing}
\subsection{Batch Processing}
\begin{itemize}
	\item Verarbeitet statische Datenmengen in festen Zeitabständen oder auf Abruf
	\item Daten liegen vollständig vor, bevor sie verarbeitet werden
	\item Höhere Latenz, dafür oft einfachere Verarbeitung und Fehlerbehandlung
	\item Gut geeignet für regelmäßige Reports, ETL-Prozesse, Backfills
	\item Beispiele: Spark Batch, Hadoop MapReduce, SQL-Jobs
\end{itemize}

\subsection{Wann Batch Processing besser ist}
\begin{itemize}
	\item Arbeiten mit großen Datenmengen in regelmäßigen Abständen (z.B. täglich, stündlich)
	\item Echtzeitreaktion nicht notwendig ist (z.B. Reporting, Data Warehousing, Archivierung)
	\item Komplexe Analysen oder Aggregationen auf vollständigen Datensätzen 
	\item Datenqualität, Wiederholbarkeit und Genauigkeit wichtiger sind als Geschwindigkeit
\end{itemize}

\subsection{Streaming Processing}
\begin{itemize}
	\item Verarbeitet Daten kontinuierlich und nahezu in Echtzeit
	\item Daten werden direkt bei Eintreffen verarbeitet
	\item Geringe Latenz, dafür komplexer in Fehlerhandling und Zustandsverwaltung
	\item Ideal für Echtzeitanalysen, Monitoring, Betrugserkennung, IoT
	\item Beispiele: Spark Structured Streaming, Apache Flink, Kafka Streams
\end{itemize}

\subsection{Wann Streaming Processing besser ist}
\begin{itemize}
	\item Auf Ereignisse muss sofort reagiert werden (z.B. Alarme, Benachrichtigungen, Live-Metriken)
	\item Daten kontinuierlich einlaufen (z.B. Logdaten, Sensoren, Webtraffic)
	\item Niedrige Latenz (Echtzeit oder Near-Real-Time)
	\item Laufendes Überwachen oder Steuern von Systemen
\end{itemize}


\newpage
\section{Spark}
\subsection{Basics}
\begin{itemize}
	\item Open Source computing engine for parallel data processing on computer clusters
	\item Cluster of computers pools the resources of many machines together to use all cumulative resources as if they were a single computer
	\item A Spark application is submitted to these cluster managers which will grant resources to the application
\end{itemize}

\subsection{Spark Application}
\begin{itemize}
	\item Spark applications consists of a driver process and a set of executor processes
	\item Driver:
	\begin{itemize}
		\item Creates SparkContext and SparkSession
		\item Runs the \texttt{main()} function, sits on a node in the cluster, and is responsible for three things:
		\begin{itemize}
			\item Maintaining information about the Spark application
			\item Responding to a user's program or input
			\item Analyzing, distributing, and scheduling work across the executors
		\end{itemize}
	\item The Driver process is the heart of the Spark application and maintains all relevant information during the lifetime of the application
	\end{itemize}
	\item Executor (on worker node):
	\begin{itemize}
		\item Responsible for carrying out the work that the driver assigns them
		\item Each executor is responsible for 2 things:
		\begin{itemize}
			\item Executing code in parallel (one per executor core) assigned to it by the driver
			\item Reporting the state of computation on that executor back to the driver
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Lazy Evaluation}
\begin{itemize}
	\item Spark waits until the very last moment to execute the graph (DAG) of computation instructions
	\item In Spark, a plan of transformations (DAG) is built up that is to be applied to the data rather than modifying the data immediately
	\item By waiting, Spark compiles this plan from the raw DataFrame transformations to a streamlined physical plan (DAG) that will run as efficiently as possible across the cluster
	\item As result, Spark can optimize the entire data flow from end to end (predicate pushdown)
\end{itemize}

\subsection{DAG}
\begin{itemize}
	\item Logical representation of a Spark computation
	\item Consists of a series of transformations (narrow/wide) applied to a DataFrame
	\item Ensures that computations are structured efficiently and executed in the right order
\end{itemize}

\subsection{Transformation}
\begin{itemize}
	\item Operation creating a new DataFrame from an existing one
	\item Transformations = logical transformation plan (DAG)
	\item Narrow transformations:
	\begin{itemize}
		\item Operation where each partition is used by at most one partition in the output
		\item No data shuffling between nodes, thus more efficient (e.g., \texttt{filter()})
		\item Executed in a single stage
	\end{itemize}
	\item Wide Transformations:
	\begin{itemize}
		\item Operation where data from one partition is used by multiple partitions in the output
		\item Require data shuffling across nodes, thus expensive (e.g., \texttt{groupBy()})
		\item Create new stage in the DAG
	\end{itemize}
\end{itemize}

\subsection{Action}
\begin{itemize}
	\item Action = triggers the computation
	\item An action instructs Spark to compute a result from a series of transformations (e.g., \texttt{count()})
	\item Every action starts a new job
\end{itemize}

\subsection{Job}
\begin{itemize}
	\item Series of parallel transformations and actions (complete computations) performed on a DataFrame, triggered by an action
	\item Operate on partitions of the DataFrame
	\item Consists of:
	\begin{itemize}
		\item Stages $ \to $ each job is divided into multiple stages (set of operations)
		\item Tasks $ \to $ each stage consists of multiple tasks running in parallel on executor cores
	\end{itemize}
\end{itemize}

\subsection{Stage}
\begin{itemize}
	\item Collection of parallel tasks separated by shuffle boundaries
	\item Narrow transformations can be executed in the same stage
	\item Wide transformations require data shuffling and create new stages
\end{itemize}

\subsection{Task}
\begin{itemize}
	\item Each Task runs on and processes one partition of the data
\end{itemize}

\subsection{Partition}
\begin{itemize}
	\item To allow every executor to perform work in parallel, Spark breaks up the data into partitions
	\item Partition: Collection of rows that sit on one physical machine in the cluster
	\item Represents how the data is physically distributed across the cluster of machines during execution
	\item One partition = parallelism of one
	\item Many partitions but only one executor = parallelism of one
\end{itemize}

\newpage
\subsection{Summary}
\begin{itemize}
	\item SparkContext:
	\begin{itemize}
		\item Core entry point for Spark operations
		\item Created automatically inside a SparkSession
	\end{itemize}
	\item SparkSession:
	\begin{itemize}
		\item Entry point for a Spark application
		\item Provides access to DataFrames, datasets, and SQL functionality
		\item Created using: \texttt{SparkSession.builder.appName().getOrCreate()}
	\end{itemize}
	\item Cluster:
	\begin{itemize}
		\item Group of interconnected machines (nodes) working together to process large-scale data in parallel
		\item Managed by cluster manager like YARN, Kubernetes, or Mesos
	\end{itemize}
	\item Node: 
	\begin{itemize}
		\item Virtual or physical machine in the cluster
	\end{itemize}
	\item Driver node:
	\begin{itemize}
		\item Virtual or physical machine where the driver runs
		\item Central coordinator of a Spark application
		\item Creates SparkContext, schedules tasks, and collects results from executors
		\item Allocates resources to executors
		\item Converts Spark code into a DAG of tasks
	\end{itemize}
	\item Driver core:
	\begin{itemize}
		\item Controls how many CPU cores are allocated to the Spark driver for managing the Spark application and scheduling tasks
	\end{itemize}
	\item Driver memory:
	\begin{itemize}
		\item RAM allocated to the Spark driver for storing metadata, job information, and small collected results
	\end{itemize}
	\item Worker node:
	\begin{itemize}
		\item Virtual or physical machine in the cluster that contains and runs multiple executors to process data in parallel
	\end{itemize}
	\item Executor:
	\begin{itemize}
		\item Process running on a worker node
		\item Responsible for executing tasks and storing intermediate data in memory or disk
	\end{itemize}
	\item Executor core:
	\begin{itemize}
		\item Number of CPU cores allocated to each executor
		\item Defines how many Tasks an executor can run in parallel
	\end{itemize}
	\item Executor memory:
	\begin{itemize}
		\item RAM allocated to each executor for performing computations and storing cached data
	\end{itemize}
\end{itemize}

\subsection{Spark vs. Python}
\subsubsection{Python}
\begin{itemize}
	\item Für kleine bis mittelgroße Datenmengen, die in den lokalen Speicher passen
	\item Bei schnellen Analysen, Exploration und Prototyping
	\item Wenn viele Bibliotheken (z.B. Pandas, NumPy, scikit-learn) gebraucht werden
	\item Ideal für lokale Entwicklungsumgebungen und Skripting 
\end{itemize}

\subsubsection{Spark}
\begin{itemize}
	\item Bei sehr großen Datenmengen, die nicht in den Arbeitsspeicher passen
	\item Wenn Daten verteilt verarbeitet oder aus verteilten Quellen gelesen werden sollen
	\item Für skalierbare ETL-Prozesse, Datenpipelines oder wiederkehrende Batch-Jobs
	\item In Cluster-Umgebungen (z.B. Databricks, EMR, Kubernetes) mit mehreren Knoten
\end{itemize}


\section{Clean Code}
\begin{itemize}
	\item Aussagekräftige Namen verwenden: Variablen, Funktionen und Klassen sollten klar und präzise beschreiben, was sie tun 
	\item Funktionen klein und fokussiert halten: Jede Funktion sollte nur eine Aufgabe erfüllen, z.B. eine Transformation in Spark oder eine bestimmte Berechnung
	\item Code modularisieren und wiederverwenden: Wiederkehrende Logik in Funktionen oder Klassen auslagern, um Duplikate zu vermeiden (z.B. wiederverwendbare UDFs in Spark)
	\item Kommentare und Docstrings nutzen: Komplexe Logik mit kurzen Kommentaren erklären und Funktionen mit Docstrings dokumentieren (speziell bei Transformationen)
	\item Vermeiden magischer Zahlen und Strings: Statt harte Werte direkt im Code zu nutzen, Konstanten definieren und gut benennen
	\item Fehlerbehandlung sinnvoll einbauen: Ausnahmen abfangen, aber nicht still ignorieren; klare Fehlermeldungen geben
	\item DataFrame-Operationen lesbar schreiben: Ketten von Spark-Transformationen klar strukturieren, z.B. jede Operation in einer neuen Zeile
	\item Vermeiden zu komplexer Verschachtelungen:	Schreibe lieber mehrere einfache Schritte als einen langen komplexen Block
	\item Teste kritische Transformationen: Nutzen von Unit-Tests oder kleine Integrationstests, um Datenqualität sicherzustellen (z.B. mit pytest und Spark Testing Base)
\end{itemize}































\end{document}