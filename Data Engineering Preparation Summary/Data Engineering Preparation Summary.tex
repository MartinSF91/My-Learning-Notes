
\documentclass[11pt]{scrartcl}
\usepackage{graphicx}
\usepackage{amsmath, amssymb, textcomp}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{ucs}
\usepackage[T1]{fontenc}
\usepackage{subcaption} 		% Subfigures
\usepackage{hyperref}
\usepackage{fullpage}	
\usepackage{color}
\usepackage{enumitem}
\setlist[itemize]{noitemsep, topsep=1pt}
\usepackage[font=footnotesize]{caption}
\usepackage{draftwatermark}
\SetWatermarkText{Martin Fané}
\SetWatermarkScale{3}
\SetWatermarkColor[gray]{0.99}
\usepackage[numindex,nottoc,section]{} 


\begin{document}
\tableofcontents 

\newpage
\section{Data Engineering Storage Abstractions}
\subsection{Data Lake}
\begin{itemize}
	\item Ein Data Lake ist ein zentrales Repository, das strukturierte, semi-strukturierte und unstrukturierte Rohdaten in großem Umfang speichert
	\item Eignet sich besonders für Big-Data-Analysen und Machine Learning, erfordert aber zusätzliche Verarbeitung zur Datennutzung
\end{itemize}


\subsection{Data Warehouse}
\begin{itemize}
	\item Ein Data Warehouse ist ein zentrales, strukturiertes System zur langfristigen Speicherung und Analyse großer Datenmengen aus verschiedenen Quellen
	\item Optimiert für komplexe Abfragen und Business-Intelligence-Anwendungen
\end{itemize}


\subsection{Cloud Data Warehouse}
\begin{itemize}
	\item Ein Cloud Data Warehouse ist ein Data Warehouse, das in der Cloud gehostet wird und dadurch flexibel skalierbar und wartungsarm ist
	\item Beispiele: Snowflake, Google BigQuery oder Amazon Redshift
\end{itemize}


\subsection{Data Lakehouse}
\begin{itemize}
	\item Datenarchitektur, die die Flexibilität und Skalierbarkeit eines Data Lakes mit den strukturierten Datenmanagement- und Analysefunktionen eines Data Warehouses kombiniert
	\item Ermöglicht sowohl explorative Datenanalyse als auch strukturierte BI-Abfragen auf einer gemeinsamen Plattform
	\item ACID Transaktionen:
	\begin{itemize}
		\item Atomicity: Transaktion wird entweder vollständig ausgeführt oder gar nicht – es gibt keine halbfertigen Zustände
		\item Consistency: Transaktion überführt die Datenbank von einem konsistenten Zustand in einen anderen, wobei alle definierten Regeln (z.B. Integritätsbedingungen) eingehalten werden
		\item Isolation: Gleichzeitige Transaktionen beeinflussen sich nicht gegenseitig – jede Transaktion läuft so, als wäre sie allein im System
		\item Durability: Sobald eine Transaktion abgeschlossen ist, bleiben ihre Änderungen dauerhaft gespeichert, selbst bei Systemausfällen
	\end{itemize}
\end{itemize}


\subsection{Delta Lake}
\begin{itemize}
	\item Open-Source-Storage-Schicht, die auf Data Lakes (z.B. in S3 oder Azure Blob Storage) aufsetzt und Funktionen wie ACID-Transaktionen, Schema-Management, Zeitreisen (Versionierung) und verlässliche Upserts/Merges bietet
	\item Damit macht Delta Lake rohe Data Lakes zuverlässiger, konsistenter und besser für Analytics und Machine Learning geeignet – indem es typische Probleme von Data Lakes (wie inkonsistente Daten oder fehlende Transaktionen) löst. Es wird oft mit Apache Spark und Databricks genutzt
\end{itemize}	


\subsection{Data Mart}
\begin{itemize}
	\item Spezialisierte Teilmenge eines Data Warehouses, die auf die Anforderungen eines bestimmten Fachbereichs zugeschnitten ist
	\item Verbessert die Performance und Übersichtlichkeit für gezielte Analysen
\end{itemize}


\subsection{Data Mesh}
\begin{itemize}
	\item Dezentraler Ansatz zur Datenarchitektur, bei dem einzelne Teams für ihre eigenen Datenprodukte verantwortlich sind ("Data as a Product")
	\item Fördert Skalierbarkeit und Eigenverantwortung durch domänenorientierte Datenverwaltung
\end{itemize}


\subsection{Data Vault}
\begin{itemize}
	\item Modellgetriebene Methode zur Gestaltung von Data Warehouses, die speziell für historisierbare, skalierbare und auditierbare Datenarchitekturen entwickelt wurde
	\item Sie trennt Daten in drei Hauptkomponenten:
	\begin{itemize}
		\item Hubs (Schlüsselentitäten, z.B. Kunden-ID),
		\item Links (Beziehungen zwischen Hubs, z.B. Kunden $ \leftrightarrow $ Bestellungen)
		\item Satellites (beschreibende, historisierte Daten mit Zeitstempel, z.B. Kundenname, Adresse).
	\end{itemize}
	\item Eignet sich besonders für agile, stark wachsende Datenumgebungen mit hohem Bedarf an Nachvollziehbarkeit und Flexibilität.
\end{itemize}


\section{Datenmanagement}
\begin{itemize}
\item Übergeordnete Prozess der Erfassung, Speicherung, Organisation, Pflege und Nutzung von Daten in einem Unternehmen
\item Umfasst verschiedene Disziplinen wie Data Governance, Datenintegration, Datenqualität, Metadatenmanagement und Archivierung, um den wertschöpfenden Einsatz von Daten sicherzustellen
\item Datenintegrität: Zusammenführen und Vereinheitlichung von Daten aus verschiedenen Quellen 
\end{itemize}


\subsection{Data Governance}
\begin{itemize}
\item Bezeichnet den Rahmen aus Richtlinien, Prozessen und Verantwortlichkeiten, der sicherstellt, dass Daten im Unternehmen korrekt, sicher, einheitlich und regelkonform verwaltet werden
\item Umfasst Themen wie Datenqualität, Datenschutz, Zugriffsrechte, Compliance und Datenverantwortung, um Vertrauen und Kontrolle über Daten zu gewährleisten
\end{itemize}


\subsection{Datenqualität}
\begin{itemize}
\item Vollständigkeit $ \to $ Sind alle erforderlichen Datenfelder vorhanden und ausgefüllt?
\item Korrektheit $ \to $ Entsprechen die Daten den realen, erwarteten Werten (z.B. stimmen Postleitzahlen mit Städten überein)?
\item Konsistenz $ \to $ Sind die Daten in verschiedenen Systemen oder Tabellen widerspruchsfrei (z.B. gleicher Kundennamen in allen Datensätzen)?
\item Aktualität $ \to $ Sind die Daten aktuell bzw. zeitgerecht verarbeitet (z.B. keine veralteten Transaktionen im Reporting)?
\item Eindeutigkeit $ \to $ Gibt es doppelte Datensätze, wo es keine geben sollte (z.B. doppelte Kunden-IDs)?
\item Validität $ \to $ Entsprechen die Daten den erwarteten Formaten oder Regeln (z.B. E-Mail-Adressen im gültigen Format, Zahlen in numerischen Feldern)?
\end{itemize}


\subsection{Datenintegration}
\begin{itemize}
	\item Prozess, bei dem Daten aus verschiedenen Quellen zusammengeführt, vereinheitlicht und für eine zentrale Nutzung bereitgestellt werden
	\item Ziel ist es, konsistente, vollständige und aktuelle Informationen für Analysen, Berichte oder operative Systeme bereitzustellen
\end{itemize}


\subsection{Daten Lifecycle}
\begin{itemize}
	\item Beschreibt die verschiedenen Phasen, die Daten während ihrer Existenz durchlaufen – von der Erfassung über Speicherung, Verarbeitung, Nutzung und Weitergabe bis hin zur Archivierung oder Löschung
	\item Hilft, Daten systematisch zu verwalten, deren Qualität zu sichern und rechtliche sowie sicherheitsrelevante Anforderungen zu erfüllen	
\end{itemize}


\subsection{Data Catalog}
\begin{itemize}
	\item Zentrales Verzeichnis, das Metadaten über Datenbestände innerhalb eines Unternehmens organisiert, beschreibt und auffindbar macht
	\item Hilft Nutzern dabei, Datenquellen schnell zu finden, deren Struktur und Bedeutung zu verstehen und die Daten effizient zu nutzen – oft unterstützt durch Suchfunktionen, Tags und Datenklassifizierung
\end{itemize}


\subsection{Unity Catalog}
\begin{itemize}
	\item Einheitliches Datenverwaltungssystem von Databricks, das Zugriffskontrolle, Daten-Governance und Katalogisierung für alle Daten-Assets über verschiedene Workloads hinweg (z.B. SQL, Python, BI-Tools) ermöglicht
	\item Bietet zentrale Verwaltung von Benutzerrechten, Datenklassifikation und Lineage (Herkunftsnachverfolgung) über mehrere Workspaces und Clouds hinweg – für mehr Sicherheit, Transparenz und Zusammenarbeit in Data- und ML-Projekten
\end{itemize}


\subsection{Feature Store}
\begin{itemize}
\item Zentrale Plattform zur Speicherung, Verwaltung und Wiederverwendung von Merkmalen (Features), die für Machine-Learning-Modelle verwendet werden
\item Ermöglicht konsistente und effiziente Bereitstellung von Features sowohl für das Training als auch für die Echtzeit-Vorhersage in produktiven ML-Systemen
\end{itemize}


\section{Daten}
\subsection{Datenprodukt}
\begin{itemize}
	\item Wiederverwendbares, klar definiertes Datenartefakt (Datensatz), das für einen konkreten Anwendungsfall erstellt wird und von anderen leicht konsumiert werden kann
\end{itemize}


\subsection{Data Lineage}
\begin{itemize}
	\item Beschreibt die Herkunft, den Weg und die Transformationen von Daten über verschiedene Systeme hinweg – vom Ursprung bis zur Nutzung, etwa in Berichten oder Analysen
	\item Hilft dabei, Datenflüsse nachvollziehbar zu machen, die Datenqualität zu sichern und regulatorische Anforderungen zu erfüllen
\end{itemize}


\subsection{Daten Versionierung}
\begin{itemize}
	\item Systematische Erfassen, Speichern und Verwalten verschiedener Zustände oder Versionen von Datensätzen im Zeitverlauf
	\item Dadurch können Änderungen nachvollzogen, frühere Datenzustände wiederhergestellt und Reproduzierbarkeit in Analysen und Machine-Learning-Modellen sichergestellt werden
\end{itemize}


\subsection{Datenmodellierung}
\begin{itemize}
	\item Prozess der strukturierten Darstellung von Daten und deren Beziehungen untereinander, meist als Diagramm oder Modell
	\item Ziel ist es, ein klares, logisches Gerüst für die Speicherung, Verwaltung und Nutzung von Daten zu schaffen – typischerweise in Form von konzeptionellen, logischen und physischen Datenmodellen
\end{itemize}


\subsubsection{Methoden}
\begin{itemize}
	\item Relationales Modell:
	\begin{itemize}
		\item Grundlage für relationale Datenbanken; Daten werden in Tabellen (Relationen) mit Zeilen und Spalten organisiert
		\item Beispiel: Tabelle Kunde mit Spalten wie KundenID, Name, Adresse.
	\end{itemize}
	\item Entity-Relationship-Modell:
	\begin{itemize}
		\item Visualisiert Entitäten und deren Beziehungen zur konzeptuellen Planung von Datenstrukturen
		\item Beispiel: Entitäten \textit{Kunde} und \textit{Bestellung} sind über eine Beziehung \textit{tätigt} verbunden
	\end{itemize}
	\item Star Schema:
	\begin{itemize}
		\item Häufig im Data-Warehouse-Bereich; eine zentrale Faktentabelle ist mit mehreren Dimensionstabellen verbunden
		\item Beispiel: Faktentabelle \textit{Umsatz} mit Verbindungen zu \textit{Produkt, Zeit, Kunde, Standort}
	\end{itemize}
\end{itemize}


\subsection{Datenformate}
\begin{itemize}
	\item Parquet:
	\begin{itemize}
		\item Spaltenbasiertes Speicherformat
		\item Komprimiert und effizient für Analyse-Workloads
	\end{itemize}
	\item CVS:
	\begin{itemize}
		\item Einfaches, textbasiertes Format
		\item Weit verbreitet, aber keine Schema- oder Typinformation
	\end{itemize}
	\item JSON:
	\begin{itemize}
		\item Textbasiert, semi-strukturiert
		\item Gut für hierarchische Daten, aber größer und langsamer als binäre Formate
	\end{itemize}
	\item Delta Table:
	\begin{itemize}
		\item Baut auf dem Apache Parquet Format auf und wird durch Delta Lake erweitert, um transaktionale Konsistenz, Schema-Evolution und Zeitreisen (Versionierung) in Data Lakes zu ermöglichen.
		\item Erlaubt ACID-Transaktionen auf großen Datenmengen, was insbesondere bei Big-Data-Analysen und Machine Learning für zuverlässige, reproduzierbare Datenpipelines sorgt
	\end{itemize}
	\item XML:
	\begin{itemize}
		\item Textbasiert, hierarchisch, aber oft sehr groß und weniger performant
	\end{itemize}
\end{itemize}


\subsection{Feature Engineering}
\begin{itemize}
	\item Skalierbare und automatisierte Aufbereitung von Merkmalen (Features) für Machine-Learning-Modelle innerhalb von Datenpipelines
	\item Data Engineers bauen dafür robuste, reproduzierbare Prozesse zur Berechnung, Speicherung und Bereitstellung von Features – z.B. über Feature Stores – und sorgen dafür, dass diese Merkmale konsistent, versioniert und performant für Training und Inferenz verfügbar sind
\end{itemize}


\section{Daten Pipeline}
\subsection{ETL}
\begin{itemize}
	\item Prozess, bei dem Daten aus verschiedenen Quellen extrahiert, in ein geeignetes Format transformiert und schließlich in ein Zielsystem wie ein Data Warehouse geladen werden
	\item Dient dazu, Rohdaten für Analysen nutzbar zu machen, indem sie bereinigt, vereinheitlicht und angereichert werden
	\item Moderne ETL-Prozesse können auch in Echtzeit (Streaming-ETL) oder als ELT-Variante ablaufen, bei der die Transformation nach dem Laden erfolgt
\end{itemize}


\subsection{ETL Pipeline}
\begin{itemize}
	\item Extraktion $ \to $ ADF (Extrahieren von Daten aus verschiedenen Quellen)
	\item Transform $ \to $ Databricks und Spark (Verarbeiten, bereinigen und anreichern der Daten in skalierbaren Umgebungen)
	\item Load $ \to $ Snowflake, BigQuery (Laden der transformierten Daten in ein Cloud DWH)
	\item Orchestrierung $ \to $ Apache Airflow (Steuerung und Automatisierung des Ablaufs der gesamten ETL-Prozesse)
	\item Montoring $ \to $ Grafana, Azure Monitor (Überwachung von Performance, Ausfällen und Fehlern der Pipeline)
	\item Data Quality \& Testing $ \to $ Great Expectations (Validieren der Datenqualität durch automatisierte Tests und Regeln)
\end{itemize}


\subsection{Pipeline Montoring}
\begin{itemize}
	\item Überwachung von Datenpipelines, Systemen und Prozessen, um sicherzustellen, dass Daten zuverlässig, korrekt und zielgerecht verarbeitet werden
	\item Fehlererkennung:
	\begin{itemize}
		\item Erkennen von fehlgeschlagenen ETL-Jobs
		\item Identifikation von Datenanomalien
		\item Monitoring von Datenlatenzen
	\end{itemize}
	\item Performance Überwachung:
	\begin{itemize}
		\item Laufzeit von Pipelines
		\item Ressourcenverbrauch
		\item Datenvolumen und Verarbeitungsraten
	\end{itemize}
	\item Sicherstellen der Datenqualität:
	\begin{itemize}
		\item Validierung von Daten gegen Regeln
		\item Schema Überwachung
	\end{itemize}
	\item Transparenz und Reporting:
	\begin{itemize}
		\item Dashboarding von Metriken (Grafana, Power BI)
		\item Alerting bei Schwellenwertüberschreitung
	\end{itemize}
	\item Beispiele:
	\begin{itemize}
		\item Apache Airflow Monitoring: Überwacht den Status von DAGs, ob Tasks fehlschlagen oder hängenbleiben
		\item Snowflake oder BigQuery Monitoring: Überwachung von Query-Ausführungszeiten und -Kosten
		\item Streaming Monitoring: Sicherstellen, dass Messages rechtzeitig und vollständig verarbeitet werden
	\end{itemize}
\end{itemize}


\subsection{CI/CD}
\begin{itemize}
	\item CI: Automatisches Testen und Bauen bei jeder Codeänderung
	\item CDelivery: Automatisches Ausliefern in eine Staging-Umgebung
	\item CDeployment: Automatisches Ausliefern bis in die Produktion (ohne manuelle Eingriffe)	
\end{itemize}


\subsection{Orchestrierung}
\begin{itemize}
\item Zentrale Steuerung, Planung und Überwachung von Datenprozessen (wie ETL- oder ELT-Jobs), sodass sie in der richtigen Reihenfolge, zur richtigen Zeit und abhängig voneinander ausgeführt werden
\item Sorgt für Automatisierung, Fehlerbehandlung, Wiederholbarkeit und Effizienz komplexer Datenpipelines – typischerweise mit Tools wie Apache Airflow, Prefect oder Dagster
\end{itemize}


\subsection{Logging}
\begin{itemize}
\item Strukturiertes Erfassen und Speichern von Informationen über den Ablauf, Status und mögliche Fehler von Datenprozessen (z.B. ETL-Jobs, Pipelines, APIs)
\item Dient der Überwachung, Fehlerdiagnose, Auditierbarkeit und Performanceanalyse von Datenflüssen – oft zentral für Debugging und Betriebssicherheit
\end{itemize}


\subsection{Data Analytics Konzepte}
\begin{itemize}
	\item EDA
	\item Deskriptive, diagnostische, prädiktive und präskriptive Analysen
	\item KPI-Tracking: Überwachung von Leistungskennzahlen
\end{itemize}


\subsection{Dokumentation}
\begin{itemize}
	\item Datenquellen:
	\begin{itemize}
		\item Herkunft (Systeme, APIs, Dateien)
		\item Zugriffsmethoden und -rechte
		\item Aktualisierungsfrequenz
	\end{itemize}
	\item ETL-/ELT-Prozesse:
	\begin{itemize}
		\item Datenflüsse und Pipeline-Übersichten
		\item Verwendete Transformationslogik (SQL, dbt, Spark etc.)
		\item Abhängigkeiten zwischen Schritten
	\end{itemize}
	\item Datenmodelle und Schemata:
	\begin{itemize}
		\item Tabellen, Views, Spaltenbeschreibungen
		\item Beziehungen zwischen Entitäten (z.B. ER-Diagramme)
		\item Versionierung von Modellen
	\end{itemize}
	\item Datenqualitätsregeln:
	\begin{itemize}
		\item Validierungschecks
		\item Testdefinitionen (z.B. dbt tests, Great Expectations)
	\end{itemize}
	\item Zugriffs- und Sicherheitsrichtlinien:
	\begin{itemize}
		\item Rollen und Berechtigungen
		\item Datenklassifizierung (z.B. PII, öffentlich, vertraulich)
	\end{itemize}
	\item Monitoring \& Alerting:
	\begin{itemize}
		\item Überwachte Metriken
		\item Fehlerbehandlung und Wiederanläufe
		\item SLAs und SLOs
	\end{itemize}
	\item Code- und Tool-Dokumentation:
	\begin{itemize}
		\item Repos, Technologien, Abhängigkeiten
		\item Setup-Anleitungen und Deployment-Prozesse
	\end{itemize}
\end{itemize}

\newpage
\section{Spark}
\subsection{Basics}
\begin{itemize}
	\item Open Source computing engine for parallel data processing on computer Clusters
	\item Cluster (Group) of computers pools the resources of many machines together to use all cumulative resources as if they were a single computer
	\item Spark manages and coordinates the execution of tasks on data across a Cluster of computers
	\item Cluster of machines is managed by a Cluster Manager like YARN, or Mesos
	\item A Spark Application is submitted to these Cluster Managers, which will grant resources to the application
\end{itemize}

\subsection{Spark Application}
\begin{itemize}
	\item Spark Applications consists of a Driver process and a set of Executor processes
	\item Driver:
	\begin{itemize}
		\item Creates SparkContext and SparkSession
		\item Runs the \texttt{main()} function, sits on a Node in the Cluster, and is responsible for three things:
		\begin{itemize}
			\item Maintaining information about the Spark Application
			\item Responding to a user’s program or input
			\item Analyzing, distributing, and scheduling work across the Executors
		\end{itemize}
	\item The Driver process is the heart of the Spark Application and maintains all relevant information during the lifetime of the application
	\end{itemize}
	\item Executor (on Worker Node):
	\begin{itemize}
		\item Responsible for carrying out the work that the Driver assigns them
		\item Each Executor is responsible for 2 things:
		\begin{itemize}
			\item Executing code in parallel (one per Executor Core) assigned to it by the Driver
			\item Reporting the state of computation on that Executor back to the Driver
		\end{itemize}
	\end{itemize}
\end{itemize}































\end{document}